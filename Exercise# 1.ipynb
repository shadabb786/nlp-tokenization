{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNU13UnS1iXJQZPZoQYm9LC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install nltk\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WGMb435WysKl","executionInfo":{"status":"ok","timestamp":1730666103673,"user_tz":-60,"elapsed":4700,"user":{"displayName":"Shadab Afridi","userId":"07104454539450663796"}},"outputId":"7aaed283-97bb-4c28-ac62-df1b00373d5e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.probability import FreqDist"],"metadata":{"id":"Ow6nw6YezA0Z","executionInfo":{"status":"ok","timestamp":1730666312754,"user_tz":-60,"elapsed":324,"user":{"displayName":"Shadab Afridi","userId":"07104454539450663796"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WAcVvxzjxA9t","executionInfo":{"status":"ok","timestamp":1730666572679,"user_tz":-60,"elapsed":922,"user":{"displayName":"Shadab Afridi","userId":"07104454539450663796"}},"outputId":"26213299-ddc0-4ecf-bf6a-7e82dee6ebb9"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["text = \"Natural language processing (NLP) is a field of artificial intelligence. It enables machines to understand and interpret human language. NLP has many applications such as chatbots and text summarization. Tokenization is an important step in text preprocessing.\"\n"],"metadata":{"id":"nMTF7oj3zbqx","executionInfo":{"status":"ok","timestamp":1730666575596,"user_tz":-60,"elapsed":310,"user":{"displayName":"Shadab Afridi","userId":"07104454539450663796"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["tokens = word_tokenize(text)"],"metadata":{"id":"zEfwJmYIzmbZ","executionInfo":{"status":"ok","timestamp":1730667011702,"user_tz":-60,"elapsed":314,"user":{"displayName":"Shadab Afridi","userId":"07104454539450663796"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["print(\"Tokens:\" , tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tj3OmesizwSj","executionInfo":{"status":"ok","timestamp":1730667015580,"user_tz":-60,"elapsed":817,"user":{"displayName":"Shadab Afridi","userId":"07104454539450663796"}},"outputId":"87a93fa9-8ccf-4ae5-c9b2-37bc3508aebd"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', 'and', 'interpret', 'human', 'language', '.', 'NLP', 'has', 'many', 'applications', 'such', 'as', 'chatbots', 'and', 'text', 'summarization', '.', 'Tokenization', 'is', 'an', 'important', 'step', 'in', 'text', 'preprocessing', '.']\n"]}]},{"cell_type":"code","source":["num_tokens = len(tokens)\n","print(\"Total Numbers of Tokens:\", num_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJQkAqgMzylC","executionInfo":{"status":"ok","timestamp":1730667019275,"user_tz":-60,"elapsed":318,"user":{"displayName":"Shadab Afridi","userId":"07104454539450663796"}},"outputId":"9e3bdac7-c792-4b22-d2d7-cce1c6e264d7"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Numbers of Tokens: 43\n"]}]},{"cell_type":"code","source":["tokenss = FreqDist(tokens)\n","print(\"Total number of tokens:\", tokenss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k31kfQDn0S6v","executionInfo":{"status":"ok","timestamp":1730667022259,"user_tz":-60,"elapsed":333,"user":{"displayName":"Shadab Afridi","userId":"07104454539450663796"}},"outputId":"6dba6b2e-a163-4b86-ce91-d061a8058220"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of tokens: <FreqDist with 35 samples and 43 outcomes>\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"lFX_zuU31E8G"},"execution_count":null,"outputs":[]}]}